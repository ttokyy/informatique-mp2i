\input{../commands_alt.tex}

\usepackage{stmaryrd}

\newcommand{\ntoinf}{\underset{n \to +\infty}{\xrightarrow{\hspace*{10mm}}}}

\begin{document}
\eqskip{2mm}

\title{Complexité pire cas}

\section{Introduction}

	L'efficacité d'un programme ne se mesure pas à la concision, ni à la clarté de son code source, mais à la consommation en ressources (temps, espace mémoire...) de son exécution.
			\nt
	Dans ce chapitre, on se concentrera sur la complexité temporelle des fonctions, c'est-à-dire la consommation en temps de leur appel en fonction des valeurs des arguments ou de leurs tailles.
			\nt
	Pour évaluer cette consommation en temps, on dénombre les opérations élémentaires qui seront effectuées à l'appel de la fonction (additions, multiplications, comparaisons...). Ce nombre dépend évidemment des valeurs des arguments à l'appel.
			\vs{2.5}
		
	\begin{Exemple}
		\'Etudions les complexités de quatre fonctions différentes qui, toutes, prennent en argument \((a,b,c,d,n) \in \bb{Z}^5\) et testent si \(n \in [a..b]+[c..d]\) (\emph{cf.} annexe de ce chapitre).
			\nt
		Si on note \(\left\{ \!\!\! \begin{tabular}[h]{l} \(l_1 = \card ([a..b]) = \max(b-a+1,0)\) \\ \(l_2 = \card ([c..d]) = \max(d-c+1,0)\) \end{tabular} \right.\)\!\!\!,  alors : \\[1mm]
		\hspace*{5mm} \bdot \texttt{est\_somme\_naif} réalise de l'ordre de \(l_1 \times l_2\) opérations \\
		\hspace*{5mm} \bdot \texttt{est\_somme\_naif\_bis} en réalise aussi de l'ordre de \(l_1 \times l_2\) (dans le pire cas) \\
		\hspace*{5mm} \bdot \texttt{est\_somme\_bis} en réalise de l'ordre de \(l_1 + l_2\) \\
		\hspace*{5mm} \bdot \texttt{est\_somme\_ter} en réalise de l'ordre de 1.
	\end{Exemple}
			\vs{2}
		
	\begin{Remarque}
		On peut obtienir le temps d'exécution d'un exécutable, chronométré par bash, en tapant la commande \texttt{time ./\textcolor{blue}{executable}} dans le terminal (\emph{cf.} annexe pour les résultats sur l'exemple précédent).
	\end{Remarque}
		
			\vs{2}
	Pour évaluer la complexité d'une fonction, on s'intéresse ici au pire cas, c'est-à-dire que pour une taille donnée, on compte le nombre d'opérations élémentaires des entrées qui maximisent ce nombre. \nt
		%
	Pour \(t\) une taille d'entrées, on regarde donc :
		\[
			f(t) = \max_{e \text{ entrée de taille } t} \, g(e)
		\]
	où \(g(e)\) donne le nombre d'opérations élémentaires de la fonction sur l'entrée \(e\).
			\vs{2}
	\begin{Exemple}
		Dans l'exemple précédent, \texttt{est\_somme\_naif} et \texttt{est\_somme\_naif\_bis} ont donc la même complexité pire cas, même si on peut s'attendre à ce que la dernière fasse en général moins d'opérations à l'appel que la première.
	\end{Exemple}
			\vs{2}
	Pour exprimer l'ordre de grandeur de la complexité d'une fonction, on utilise les notations de Landau, que l'on définit dans la partie qui suit.
	
\section{Notations de Landau}

	Les notations de Landau permettent de comparer les comportements asymptotiques de deux suites, c'est-à-dire leur comportement à l'infini, et ce en omettant les constantes multiplicatives.
	
	\pagebreak
	\begin{Definitions}[notations de Landau]
		Soient \(u, v \in \bb{N}^\bb{N}\) deux suites. On a :
		\begin{align*}
			& \text{\bdot} u \in \rm{O}(v) \iff \exists \, C \in \bb{R}^{+*}, \, \exists \, n_0 \in \bb{N} \text{ tels que } \forall \, n \in \bb{N}, \, n \geq n_0 \Longrightarrow u_n \leq C v_n \\
			& \text{\bdot} u \in \Omega(v) \iff \exists \, C \in \bb{R}^{+*}, \, \exists \, n_0 \in \bb{N} \text{ tels que } \forall \, n \in \bb{N}, \, n \geq n_0 \Longrightarrow u_n \geq C v_n \\
			& \text{\bdot} u \in \Theta(v) \iff \exists \, (C_1,C_2) \in (\bb{R}^{+*})^2, \, \exists \, n_0 \in \bb{N} \text{ tels que } \forall \, n \in \bb{N}, \, n \geq n_0 \Longrightarrow C_1v_n \leq u_n \leq C_2v_n
		\end{align*}
	Si \(u \in \rm{O}(v)\), on dit que \(u\) est dominée par \(v\). \\
	Si \(u \in \Theta(v)\), on dit que \(u\) et \(v\) sont équivalentes (à ne pas confondre avec ``\(u \sim v\)'' en maths).
	\end{Definitions}
	
	\vs{2}
	\begin{Remarque} \(u \in \rm{O}(v) \iff v \in \Omega(u)\) \\
	\hspace*{24.5mm} \(u \in \Theta(v) \iff (u \in \rm{O}(v)\) et \(u \in \Omega(v)) \iff v \in \Theta(u)\). \end{Remarque} \vspace{2mm}
	%
	\begin{center}
		\textit{Afin d'alléger les notations, on se permettra désormais, pour \(((u_n)_{n \in \bb{N}}, (v_n)_{n \in \bb{N}}) \in (\bb{N}^\bb{N})^2\), d'écrire que \(u_n \in \rm{O}(v_n)\), que \(u_n \in \Omega(v_n)\) ou que \(u_n \in \Theta(v_n)\).}
	\end{center}
	%
	\vspace{2mm}
	\begin{Exemple} \((3n+2)_{n \in \bb{N}} \in \rm{O}(n), \, \rm{O}(n^2), \, \rm{O}(n^3)\) car \(\forall \, n \geq 2, \, 3n+2 \leq 4n \leq 4n^2 \leq 4n^3\). \\[2mm]
		%
	Par contre \(3n+2 \notin \Theta(n^2)\). En effet, si c'était le cas, on aurait en particulier \(3n+2 \in \Omega(n^2)\) donc il existerait \(n_0 \in \bb{N}^*\) et \(C \in \bb{R}^{+*}\) tels que
		\[\forall \, n \geq n_0, \, 3n+2 \geq Cn^2 \text{ soit } \displaystyle \frac{3n+2}{n^2} \geq C\]
	ce qui est impossible car \(\displaystyle \frac{3n+2}{n^2} = \frac{3}{n} + \frac{2}{n^2} \ntoinf 0\). \end{Exemple}
	
	\eqskip{1mm}
	\begin{Propriete}
		Les relations binaires \(\cal{R}\) et \(\cal{R}'\) définies pour \((u,v) \in (\bb{N}^\bb{N})^2\) par
			\[
				u \, \cal{R} \, v \iff u \in \rm{O}(v) \qquad \qquad \qquad u \, \cal{R'} \, v \iff u \in \Omega(v)
			\]
		sont transitives.
	\end{Propriete}
	\eqskip{2mm}
	
	\vs{2}
	\begin{Preuve}
		On utilise le fait que \[
			\left. \begin{tabular}[h]{l}
				\(\forall \, n \geq n_0, \, u_n \leq Cv_n\) \\
				\(\forall \, n \geq n_0', \, v_n \leq C'w_n\)
			\end{tabular} \!\!\! \right\} \Longrightarrow
		\forall \, n \geq \max(n_0,n_0'), \, u_n \leq CC'w_n
		\]
		pour la première équivalence, et on obtient la deuxième par le même principe (appliqué dans l'autre sens).
	\end{Preuve}
	%
	\uplabel{Corollaire :} La relation \(\widehat{\cal{R}}\) définie pour \((u,v) \in (\bb{N}^\bb{N})^2\) par \(u \,\widehat{\cal{R}}\, v \iff u \in \Theta(v)\) est transitive. En fait, on vérifie aisément que c'est même une relation d'équivalence.
	
	
	\subsection{Conditions suffisantes}
		
		\begin{Proprietes}
			Soient \(u, v \in \bb{N}^\bb{N}\). On suppose que \(v\) ne s'annule pas. Alors, \\
				\hspace*{5mm} \i Si \(\displaystyle \frac{u_n}{v_n} \ntoinf 0\), alors \(\left\{ \!\!\! \begin{tabular}[h]{l}
					\(u \in \rm{O}(v)\) \\
					\(u \notin \Theta(v)\)
				\end{tabular} \right.\) \\
					%
				\hspace*{5mm} \ii S'il existe \(\ell \in \bb{R}^{+*}\) tel que \(\displaystyle \frac{u_n}{v_n} \ntoinf \ell\), alors \(u \in \Theta(v)\) \\
					%
				\hspace*{5mm} \iii Si \(\displaystyle \frac{u_n}{v_n} \ntoinf +\infty\), alors \(u \notin \rm{O}(v)\).
		\end{Proprietes}
		%
		\begin{Remarque}
			Dans le cas \iii\!\!, on a aussi \(u \in \Omega(v)\), mais cette caractérisation nous intéresse moins pour l'étude de complexités puisque l'on cherche à écrire des algorithmes ou des fonctions de complexité la plus faible possible.
		\end{Remarque}
		%
		\begin{Remarque}
			Attention, la réciproque de la propriété \i est fausse, les suites \(u\) et \(v\) définies ci-dessous en constituent un contre-exemple : \vspace{-2mm}
			\begin{align*}
			 \forall \, n \in \bb{N}, \,\, & u_n = 1+(-1)^n\times \frac{1}{2} =
			 	\left\{ \!\!\! \begin{tabular}[h]{l}
			 		3/2 \text{ si \(n\) pair} \\
			 		1/2 \text{ si \(n\) impair}
			 	\end{tabular} \right. \\
		 		& v_n = 1+(-1)^{n+1}\times \frac{1}{2} =
		 		\left\{ \!\!\! \begin{tabular}[h]{l}
		 			1/2 \text{ si \(n\) pair} \\
		 			3/2 \text{ sinon}
		 		\end{tabular} \right.
			\end{align*}
		
		\vspace{-3.5mm} En effet, \(u_n/v_n\) n'a pas de limite (ne converge pas et ne diverge pas non plus vers \(+\infty\)), et pourtant \(u \in \Theta(v)\) puisque \(u, v \in \Theta(1)\) et que \(\widehat{\cal{R}}\) est transitive et symétrique.
	\end{Remarque}

\section{Sommes utiles}

	\setlength{\tabcolsep}{20pt}
	Voici les comportements asymptotiques de quelques sommes usuelles : \\[2mm] \hspace*{10mm} \begin{tabular}[t]{ll}
		\(\displaystyle \text{\bdot} \sum_{i=1}^n i = \frac{n(n+1)}{2} \in \Theta(n^2)\) & \(\text{\bdot} \displaystyle \sum_{i=1}^n 2^i = 2^{n+1}-1 \in \Theta(2^n)\) \\
		\(\text{\bdot} \displaystyle \sum_{i=1}^n i^2 = \frac{n(n+1)(2n+1)}{6} \in \Theta(n^3)\) & \(\text{\bdot} \displaystyle \sum_{i=1}^n \frac{1}{2^i} = 1 - \frac{1}{2^n} \in \Theta(1)\) \\
		\(\displaystyle \text{\bdot} \sum_{i=1}^n \frac{1}{i} \in \Theta(\ln n)\) & \(\text{\bdot} \displaystyle \sum_{i=1}^n \log i \in \Theta(n\log n) \ (*)\)
	\end{tabular} \vs{2}
	%
	\begin{Preuve}[$(*)$]
		Pour tout \(n \in \bb{N}^*\), on a :
			\begin{align*}
				S_n = \sum\nolimits_{i=1}^n \log i & = \sum\nolimits_{i=1}^{\lfloor n/2 \rfloor} \log i + \sum\nolimits_{i=\lfloor n/2 \rfloor+1}^n \log i \\[-0.5mm]
				& \geq \sum\nolimits_{i=\lfloor n/2 \rfloor+1}^n \underbrace{\log(i)}_{\geq \log(n/2)} \\[-1mm]
				& \geq \big(\underbrace{\lfloor n/2 \rfloor -1}_{\geq n/4}\big)(\log(n) - \log(2)) \\[-2mm]
				& \geq \frac{n}{4} (\log(n) - \log(2)) \text{ pour \(n\) assez grand} \\
				& = \underbrace{\frac{1}{4}n\log(n) - \frac{n\log 2}{4}}_{:= u_n}
			\end{align*}
		
		\vs{-2}
		Ainsi, \(S_n \in \Omega(u_n)\). Or \(u_n \in \Theta(n \log n)\), donc en particulier \(u_n \in \Omega(n \log n) \), puisque
		\[
			\displaystyle \frac{u_n}{n\log n} = \frac{\frac{n}{4} (\log(n) - \log(2))}{n\log n} = \frac{1}{4}\Big(1 - \frac{\log 2}{\log n} \Big) \ntoinf \frac{1}{4},
		\]
		Par transitivité de la relation \(\cal{R}'\) définie plus haut, on a alors \(S_n \in \Omega(n \log n)\). \\[2mm]
			%
		D'autre part, \(\displaystyle \sum\nolimits_{i=1}^n \log(i) \leq n \log(n)\) donc \(S_n \in \rm{O}(n\log n)\) ce qui achève la démonstration.
	\end{Preuve}

\end{document}